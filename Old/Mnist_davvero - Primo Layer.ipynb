{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f9cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Identity\n",
    "import geoopt\n",
    "from geoopt.optim import RiemannianSGD, RiemannianAdam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from torch.func import grad\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import rotate\n",
    "from scipy.linalg import lstsq, null_space\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cd28e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60575001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(d): # questa è  rot_mat\n",
    "    mat = np.zeros(d**4).reshape(d**2,d**2)\n",
    "    for j in range(d):\n",
    "        for i in range(d):\n",
    "            k = (i+1)*d-j\n",
    "            h = i+(j)*d+1\n",
    "\n",
    "            mat[h-1][k-1] = 1\n",
    "    return mat\n",
    "\n",
    "def W(output_dim, d = 4):\n",
    "    kernel = null_space((np.identity(d*d) -  G(d)).T).T\n",
    "    W = []\n",
    "    for j in range(output_dim):\n",
    "        vec = np.zeros(d**2)\n",
    "        for k in kernel:\n",
    "            vec = np.random.random()*k + vec\n",
    "        W.append(list(vec))\n",
    "    W = np.array(W)/len(kernel)\n",
    "    return np.array(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f1d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 28, 28])\n",
      "Train: X=torch.Size([10, 28, 28]), y=torch.Size([10])\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "\n",
    "trainX = trainX[:10]\n",
    "trainy = trainy[:10]\n",
    "\n",
    "trainy = torch.Tensor(trainy).to(device)\n",
    "trainX = trainX / 256\n",
    "testX = testX / 256\n",
    "\n",
    "trainX = torch.Tensor(trainX).to(device)\n",
    "print(trainX.shape)\n",
    "trainX_90 = rotate(img = trainX, angle = 90)\n",
    "\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e6492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(weights_list):\n",
    "    flatten_weights = []\n",
    "    for weight in weights_list:\n",
    "        flatten_weights.append(weight.flatten())\n",
    "        \n",
    "    return torch.concatenate(flatten_weights)\n",
    "\n",
    "def deflatten(weights_flat, shapes):\n",
    "    weigths_list = []\n",
    "    shape_counter = 0\n",
    "    for shape in shapes:\n",
    "        weight = torch.Tensor(weights_flat[shape_counter: shape_counter + np.prod(shape)])\n",
    "        shape_counter = shape_counter + np.prod(shape)\n",
    "        \n",
    "        weigths_list.append(weight.reshape(shape))\n",
    "            \n",
    "    return weigths_list\n",
    "\n",
    "# Funzione che definisce la varietà: sfera unitaria centrata in 0\n",
    "def g(thetas):\n",
    "    \n",
    "    W1, b1, W2, b2, W3, b3 = deflatten(thetas, model.get_shapes())\n",
    "    I_minus_G = torch.from_numpy(np.identity(28*28) - G(28)).to(device).float()\n",
    "    norm = (W1.requires_grad_(True) @ I_minus_G).norm()**2\n",
    "    \n",
    "        \n",
    "#     # Zero grad\n",
    "#     for param in model.parameters():\n",
    "#         param.grad = torch.zeros(param.shape).to(device)\n",
    "        \n",
    "    # Compute gradient\n",
    "    norm.backward()\n",
    "    \n",
    "    return norm # sfera unitaria\n",
    "\n",
    "\n",
    "def dg(thetas):\n",
    "    _ = g(thetas)\n",
    "    def_grad = [param.grad for param in model.parameters()]\n",
    "    \n",
    "    \n",
    "    return flatten(def_grad)\n",
    "\n",
    "# # Gradiente di f\n",
    "# def dg(thetas):\n",
    "#     gradient = grad(g)(thetas)\n",
    "#     return gradient #+ 1e-5*(gradient.norm()==0)\n",
    "\n",
    "\n",
    "# Classe LevelSetManifold già implementata sopra\n",
    "class LevelSetManifold(geoopt.manifolds.Manifold):\n",
    "    \n",
    "    ndim = 1\n",
    "    name = \"Caste\"\n",
    "    \n",
    "    def __init__(self, f, df, lr_proj = 1):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "        self.lr_proj = lr_proj\n",
    "\n",
    "    def _check_point_on_manifold(self, x, atol=1e-7, rtol=1e-7):\n",
    "        return torch.abs(self.f(x)) < atol\n",
    "\n",
    "    def _check_vector_on_tangent(self, x, u, atol=1e-7, rtol=1e-7):\n",
    "        grad_f = self.df(x)\n",
    "        return torch.abs(u @ grad_f).sum() < atol\n",
    "    \n",
    "    def projx(self,x):\n",
    "        if self._check_point_on_manifold(x):\n",
    "            return x\n",
    "        for r in range(100):\n",
    "            x = self.single_projx(x)\n",
    "            if r%50==0:\n",
    "                print(\"g: \", self.f(x))\n",
    "                print(\"dg: \", self.df(x).norm())\n",
    "            if r == 99:\n",
    "                print(f\"Retraction applied {r + 1} times\")\n",
    "            if self._check_point_on_manifold(x):\n",
    "                print(f\"Retraction applied {r + 1} times\")\n",
    "                break\n",
    "        return x\n",
    "    \n",
    "    def single_projx(self, x):\n",
    "        \"\"\"\n",
    "        Retraction\n",
    "        \"\"\"\n",
    "        grad_f = self.df(x)\n",
    "        f_val = self.f(x)\n",
    "        return x - 1*self.lr_proj*(f_val / grad_f.norm()**2 * grad_f)\n",
    "\n",
    "    def proju(self, x, u):\n",
    "        \"\"\"\n",
    "        Projected gradient\n",
    "        \"\"\"\n",
    "        print(\"Sono stato chiamato\")\n",
    "        grad_f = self.df(x)\n",
    "        return u - 1*(u @ grad_f) / grad_f.norm()**2 * grad_f\n",
    "\n",
    "    def inner(self, x, u, v=None):\n",
    "        if v is None:\n",
    "            v = u\n",
    "        return (u * v).sum()\n",
    "\n",
    "    def expmap(self, x, u):\n",
    "        return self.retr(x, u)\n",
    "\n",
    "    def egrad2rgrad(self, x, u):\n",
    "        return self.proju(x, u)\n",
    "\n",
    "    def retr(self, x, u):\n",
    "        x_new = x + u\n",
    "        return self.projx(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567e606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class InvariantNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden, output):\n",
    "        super(InvariantNN, self).__init__()\n",
    "        \n",
    "#         self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "    \n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.l1 = nn.Linear(self.input_dim, self.hidden)\n",
    "        self.l2 = nn.Linear(self.hidden, self.hidden)\n",
    "        self.l3 = nn.Linear(self.hidden, self.output)\n",
    "        \n",
    "        self.l1.weight.data = torch.Tensor(W(self.hidden,d=int(self.input_dim**0.5))).requires_grad_(True)\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.acc_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def get_shapes(self):\n",
    "        shapes = [(self.hidden, self.input_dim), \n",
    "                  (self.hidden,1),\n",
    "                  (self.hidden, self.hidden),\n",
    "                  (self.hidden,1),\n",
    "                  (self.output,self.hidden),\n",
    "                  (self.output,1)\n",
    "                 ]\n",
    "        return shapes\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.l2(x)    \n",
    "        x = self.tanh(x)\n",
    "        x = self.l3(x)\n",
    "\n",
    "        return  x\n",
    "    \n",
    "model = InvariantNN(28*28, 64, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3123d3ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Training phase ################\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "Epoch: 1 Loss:  2.320369243621826 g(theta):  0.0 dg:  tensor([ 0.0000,  0.0000,  0.0000,  ...,  0.1075,  0.1106, -0.0098],\n",
      "       device='cuda:0') Output:  0.07906430214643478 0.07626278698444366\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "\n",
      " tensor(6.1038, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(theta))\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m,  epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m     36\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     37\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg(theta): \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(g(theta)\u001b[38;5;241m.\u001b[39mdata),\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(model(trainX)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28msum\u001b[39m(model(trainX)[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     41\u001b[0m          )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creiamo la varietà\n",
    "manifold = LevelSetManifold(g, dg, 1)\n",
    "# Test del parametro manifold\n",
    "\n",
    "theta = flatten(model.parameters())\n",
    "theta = geoopt.ManifoldParameter(theta , manifold=manifold)\n",
    "\n",
    "# print(\"Prima: \", \"g: \", g(theta).item(), \"dg: \", dg(theta))\n",
    "\n",
    "\n",
    "# # Verifica che il parametro iniziale appartenga alla varietà\n",
    "# assert manifold._check_point_on_manifold(theta.data), \"Il punto iniziale non è sulla varietà\"\n",
    "\n",
    "# Definiamo una loss function: minimizziamo la norma quadrata\n",
    "def loss_fn(trainX):\n",
    "    return torch.nn.CrossEntropyLoss()(model(trainX), trainy.long())\n",
    "\n",
    "# Ottimizzatore Riemanniano\n",
    "optimizer = RiemannianSGD([theta], lr=0.01)\n",
    "\n",
    "# Ciclo di ottimizzazione\n",
    "g_during_train = []\n",
    "loss_history = []\n",
    "print(\"############## Training phase ################\")\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(trainX)\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"\\n\", sum(theta))\n",
    "    optimizer.step()\n",
    "    print(sum(theta), \"\\n\")\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print(\"Epoch:\",  epoch + 1, \n",
    "              \"Loss: \", loss.item(),\n",
    "              \"g(theta): \", round(float(g(theta).data),8),\n",
    "              \"dg: \", dg(theta).data,\n",
    "              #\"Sum of grad: \", sum(sum(model.l3.weight.grad)),\n",
    "              \"Output: \", sum(model(trainX)[0]).item(), sum(model(trainX)[1]).item()\n",
    "             )\n",
    "    loss_history.append(loss.item())\n",
    "    g_during_train.append(g(theta).data)\n",
    "# plt.plot(g_during_train)\n",
    "plt.plot(loss_history)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Risultato finale\n",
    "print(\"Punto finale:\", theta)\n",
    "print(\"Appartiene alla varietà?\", manifold._check_point_on_manifold(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a18c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.rand((1,28*28)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_180 = rotate(img = trainX, angle = 180)\n",
    "trainX_270 = rotate(img = trainX, angle = 270)\n",
    "\n",
    "print(model(trainX)[1])\n",
    "print(model(trainX_90)[1])\n",
    "print(model(trainX_180)[1])\n",
    "print(model(trainX_270)[1])\n",
    "\n",
    "print((model(trainX)[3] - model(trainX_180)[3]).norm()**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2, W3, b3 = deflatten(theta, model.get_shapes())\n",
    "I_minus_G = torch.from_numpy(np.identity(28*28) - G(28)).to(device).float()\n",
    "norm = (W1.requires_grad_(True) @ I_minus_G).norm()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(G(28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a015fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(I_minus_G.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(W1.cpu().detach().numpy().T @ I_minus_G.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_model = model.l1.weight.data\n",
    "plt.imshow(W_model.cpu().detach().numpy() @ I_minus_G.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1.T @ trainX[0].flatten()  - W1.T @ trainX_90[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(trainX[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters() = deflatten(thetas, model.get_shapes())\n",
    "NN1 = model(trainX)\n",
    "NN2 = model(trainX_90)\n",
    "Delta_NN = NN1 - NN2\n",
    "norm = Delta_NN.norm()**2\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6861e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

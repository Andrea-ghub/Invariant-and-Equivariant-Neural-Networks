{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d47fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import geoopt\n",
    "from geoopt.optim import RiemannianSGD, RiemannianAdam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605607e",
   "metadata": {},
   "source": [
    "# Default Geoopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbeda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un manifold di test usando geoopt predefinito, ad esempio, la sfera\n",
    "sphere = geoopt.Sphere()\n",
    "\n",
    "# Test del parametro manifold\n",
    "x = geoopt.ManifoldParameter(torch.Tensor([-0.9,50000,0]), manifold=sphere)\n",
    "print(\"Param Manifold inizializzato correttamente:\", x)\n",
    "\n",
    "\n",
    "# Proiettiamo il parametro iniziale sulla varietà\n",
    "for i in range(4):\n",
    "    x.data = sphere.projx(x.data)\n",
    "    print(i, x.norm())\n",
    "\n",
    "# Verifica che il parametro iniziale appartenga alla varietà\n",
    "assert sphere._check_point_on_manifold(x.data), \"Il punto iniziale non è sulla varietà\"\n",
    "\n",
    "# Definiamo una loss function: minimizziamo la norma quadrata\n",
    "def loss_fn(x):\n",
    "    return -x[0]  # Ad esempio, massimizzare la componente x[0]\n",
    "\n",
    "# Ottimizzatore Riemanniano\n",
    "optimizer = RiemannianSGD([x], lr=0.1)\n",
    "\n",
    "# Ciclo di ottimizzazione\n",
    "print(\"Punto iniziale:\", x)\n",
    "for epoch in range(40):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, x: {x.data}\")\n",
    "\n",
    "# Risultato finale\n",
    "print(\"Punto finale:\", x)\n",
    "print(\"Appartiene alla varietà?\", sphere._check_point_on_manifold(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ac12",
   "metadata": {},
   "source": [
    "# Custom Manifold (sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che definisce la varietà: sfera unitaria centrata in 0\n",
    "def f(x):\n",
    "    return x.norm()**2 - 1  # Sfera unitaria\n",
    "\n",
    "# Gradiente di f\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Classe LevelSetManifold già implementata sopra\n",
    "class LevelSetManifold(geoopt.manifolds.Manifold):\n",
    "    \n",
    "    ndim = 1\n",
    "    name = \"Caste\"\n",
    "    \n",
    "    def __init__(self, f, df):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "\n",
    "    def _check_point_on_manifold(self, x, atol=1e-5, rtol=1e-5):\n",
    "        return torch.abs(self.f(x)) < atol\n",
    "\n",
    "    def _check_vector_on_tangent(self, x, u, atol=1e-5, rtol=1e-5):\n",
    "        grad_f = self.df(x)\n",
    "        return torch.abs(u @ grad_f).sum() < atol\n",
    "\n",
    "    def projx(self, x):\n",
    "        grad_f = self.df(x)\n",
    "        f_val = self.f(x)\n",
    "        return x - f_val / grad_f.norm()**2 * grad_f\n",
    "\n",
    "    def proju(self, x, u):\n",
    "        grad_f = self.df(x)\n",
    "        return u - (u @ grad_f) / grad_f.norm()**2 * grad_f\n",
    "\n",
    "    def inner(self, x, u, v=None):\n",
    "        if v is None:\n",
    "            v = u\n",
    "        return (u * v).sum()\n",
    "\n",
    "    def expmap(self, x, u):\n",
    "        return self.retr(x, u)\n",
    "\n",
    "    def egrad2rgrad(self, x, u):\n",
    "        return self.proju(x, u)\n",
    "\n",
    "    def retr(self, x, u):\n",
    "        x_new = x + u\n",
    "        return self.projx(x_new)\n",
    "\n",
    "# Creiamo la varietà\n",
    "manifold = LevelSetManifold(f, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20715dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test del parametro manifold\n",
    "x = geoopt.ManifoldParameter(torch.Tensor([-0.9,50000,0]), manifold=manifold)\n",
    "print(\"Param Manifold inizializzato correttamente:\", x)\n",
    "\n",
    "\n",
    "# Proiettiamo il parametro iniziale sulla varietà\n",
    "for i in range(20):\n",
    "    x.data = manifold.projx(x.data)\n",
    "    print(i, x.norm())\n",
    "\n",
    "# Verifica che il parametro iniziale appartenga alla varietà\n",
    "assert manifold._check_point_on_manifold(x.data), \"Il punto iniziale non è sulla varietà\"\n",
    "\n",
    "# Definiamo una loss function: minimizziamo la norma quadrata\n",
    "def loss_fn(x):\n",
    "    return -x[0]  # Ad esempio, massimizzare la componente x[0]\n",
    "\n",
    "# Ottimizzatore Riemanniano\n",
    "optimizer = RiemannianSGD([x], lr=0.1)\n",
    "\n",
    "# Ciclo di ottimizzazione\n",
    "print(\"Punto iniziale:\", x)\n",
    "for epoch in range(40):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, x: {x.data}\")\n",
    "\n",
    "# Risultato finale\n",
    "print(\"Punto finale:\", x)\n",
    "print(\"Appartiene alla varietà?\", manifold._check_point_on_manifold(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9e728",
   "metadata": {},
   "source": [
    "# Custom Manifold (ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239fcb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che definisce la varietà: sfera unitaria centrata in 0\n",
    "def f(x):\n",
    "    return 2*x[0]**2 + 3*x[1]**2 + x[2]**2 - 1  # Sfera unitaria\n",
    "\n",
    "# Gradiente di f\n",
    "def df(x):\n",
    "    return torch.Tensor([4 * x[0] , 6*x[1], 2*x[2]])\n",
    "\n",
    "# Classe LevelSetManifold già implementata sopra\n",
    "class LevelSetManifold(geoopt.manifolds.Manifold):\n",
    "    \n",
    "    ndim = 1\n",
    "    name = \"Caste\"\n",
    "    \n",
    "    def __init__(self, f, df):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "\n",
    "    def _check_point_on_manifold(self, x, atol=1e-5, rtol=1e-5):\n",
    "        return torch.abs(self.f(x)) < atol\n",
    "\n",
    "    def _check_vector_on_tangent(self, x, u, atol=1e-5, rtol=1e-5):\n",
    "        grad_f = self.df(x)\n",
    "        return torch.abs(u @ grad_f).sum() < atol\n",
    "\n",
    "    def projx(self, x, lr_proj = 1):\n",
    "        grad_f = self.df(x)\n",
    "        f_val = self.f(x)\n",
    "        return x - lr_proj*(f_val / grad_f.norm()**2 * grad_f)\n",
    "\n",
    "    def proju(self, x, u):\n",
    "        grad_f = self.df(x)\n",
    "        return u - (u @ grad_f) / grad_f.norm()**2 * grad_f\n",
    "\n",
    "    def inner(self, x, u, v=None):\n",
    "        if v is None:\n",
    "            v = u\n",
    "        return (u * v).sum()\n",
    "\n",
    "    def expmap(self, x, u):\n",
    "        return self.retr(x, u)\n",
    "\n",
    "    def egrad2rgrad(self, x, u):\n",
    "        return self.proju(x, u)\n",
    "\n",
    "    def retr(self, x, u):\n",
    "        x_new = x + u\n",
    "        return self.projx(x_new)\n",
    "\n",
    "# Creiamo la varietà\n",
    "manifold = LevelSetManifold(f, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d442ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test del parametro manifold\n",
    "x = geoopt.ManifoldParameter(torch.rand(3), manifold=manifold)\n",
    "print(\"Param Manifold inizializzato correttamente:\", x)\n",
    "\n",
    "\n",
    "# Proiettiamo il parametro iniziale sulla varietà\n",
    "for i in range(4):\n",
    "    x.data = manifold.projx(x.data)\n",
    "    if i%1 == 0:\n",
    "        print(i,f(x))\n",
    "\n",
    "# Verifica che il parametro iniziale appartenga alla varietà\n",
    "assert manifold._check_point_on_manifold(x.data), \"Il punto iniziale non è sulla varietà\"\n",
    "\n",
    "# Definiamo una loss function: minimizziamo la norma quadrata\n",
    "def loss_fn(x):\n",
    "    return -x[0]  # Ad esempio, massimizzare la componente x[0]\n",
    "\n",
    "# Ottimizzatore Riemanniano\n",
    "optimizer = RiemannianSGD([x], lr=0.1)\n",
    "\n",
    "# Ciclo di ottimizzazione\n",
    "print(\"Punto iniziale:\", x)\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, x: {x.data}\")\n",
    "\n",
    "# Risultato finale\n",
    "print(\"Punto finale:\", x)\n",
    "print(\"Appartiene alla varietà?\", manifold._check_point_on_manifold(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b62993",
   "metadata": {},
   "source": [
    "# Custom Manifold (More than three parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7350d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.func import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09f292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand((10,64)).float()/10\n",
    "b = torch.rand((1,10)).float()/10\n",
    "\n",
    "W1 = torch.rand((10,10)).float()/10\n",
    "b1 = torch.rand((1,10)).float()/10\n",
    "\n",
    "W2 = torch.rand((10,10)).float()/10\n",
    "b2 = torch.rand((1,10)).float()/10\n",
    "\n",
    "W3 = torch.rand((10,64)).float()/10\n",
    "b3 = torch.rand((1,10)).float()/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495e4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo la varietà\n",
    "manifold = LevelSetManifold(f, df, 1)\n",
    "x = geoopt.ManifoldParameter(torch.rand(64).float(), manifold=manifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d057e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che definisce la varietà: sfera unitaria centrata in 0\n",
    "def f(x):\n",
    "    return ((torch.tanh(W1 @ torch.tanh(W @ x + b).T + b1.T)-torch.tanh(W2 @ torch.tanh(W3 @ x + b3).T + b2.T)).norm())**2  # Sfera unitaria\n",
    "\n",
    "# Gradiente di f\n",
    "def df(x):\n",
    "    gradient = grad(f)(x)\n",
    "    return gradient #+ 1e-5*(gradient.norm()==0)\n",
    "\n",
    "# Classe LevelSetManifold già implementata sopra\n",
    "class LevelSetManifold(geoopt.manifolds.Manifold):\n",
    "    \n",
    "    ndim = 1\n",
    "    name = \"Caste\"\n",
    "    \n",
    "    def __init__(self, f, df, lr_proj = 1):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "        self.lr_proj = lr_proj\n",
    "\n",
    "    def _check_point_on_manifold(self, x, atol=1e-5, rtol=1e-5):\n",
    "        return torch.abs(self.f(x)) < atol\n",
    "\n",
    "    def _check_vector_on_tangent(self, x, u, atol=1e-5, rtol=1e-5):\n",
    "        grad_f = self.df(x)\n",
    "        return torch.abs(u @ grad_f).sum() < atol\n",
    "    \n",
    "    def projx(self,x):\n",
    "        if self._check_point_on_manifold(x):\n",
    "            return x\n",
    "        for _ in range(25):\n",
    "            x = self.single_projx(x)\n",
    "            if self._check_point_on_manifold(x):\n",
    "                print(\"Fatto queste volte: \", _)\n",
    "                break\n",
    "        return x\n",
    "    \n",
    "    def single_projx(self, x):\n",
    "        grad_f = self.df(x)\n",
    "        f_val = self.f(x)\n",
    "        return x - self.lr_proj*(f_val / grad_f.norm()**2 * grad_f)\n",
    "\n",
    "    def proju(self, x, u):\n",
    "        grad_f = self.df(x)\n",
    "        return u - (u @ grad_f) / grad_f.norm()**2 * grad_f\n",
    "\n",
    "    def inner(self, x, u, v=None):\n",
    "        if v is None:\n",
    "            v = u\n",
    "        return (u * v).sum()\n",
    "\n",
    "    def expmap(self, x, u):\n",
    "        return self.retr(x, u)\n",
    "\n",
    "    def egrad2rgrad(self, x, u):\n",
    "        return self.proju(x, u)\n",
    "\n",
    "    def retr(self, x, u):\n",
    "        x_new = x + u\n",
    "        return self.projx(x_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e6caaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 f:  0.000371986476238817\n",
      "0 grad:  0.00476743932813406\n",
      "Fatto queste volte:  16\n",
      "50 f:  9.40897280088393e-06\n",
      "50 grad:  0.0004529641882982105\n",
      "Punto iniziale: Parameter on Caste manifold containing:\n",
      "Parameter(ManifoldParameter([ 1.6700e-01, -2.6177e-02,  1.3104e+00, -6.1981e-01,\n",
      "                    5.3287e-02,  3.7125e-01,  4.7291e-01, -2.7242e-01,\n",
      "                   -1.4850e+00, -1.3580e-01, -6.1454e-01,  8.4339e-01,\n",
      "                   -1.2752e-01,  2.9449e-01,  5.5814e-01,  9.8928e-01,\n",
      "                    6.2901e-01,  7.7562e-02, -7.5692e-01, -9.0409e-02,\n",
      "                    6.9461e-01,  2.4550e-01, -1.8421e-01,  1.1025e-01,\n",
      "                   -8.9750e-01, -3.8733e-01, -1.1085e+00, -1.0991e+00,\n",
      "                    3.9060e-02,  8.0627e-01,  1.5831e+00,  9.2555e-01,\n",
      "                    3.2890e-01,  1.5773e+00, -2.0944e-01,  9.4986e-01,\n",
      "                   -4.5910e-01,  7.1589e-02,  1.7654e+00, -1.0878e-02,\n",
      "                    2.4193e-03, -1.2824e+00, -1.8741e-01,  2.2305e-01,\n",
      "                   -5.5513e-01,  1.0245e-02, -1.0851e+00, -1.1729e+00,\n",
      "                   -1.2138e-01, -9.1045e-01,  6.6216e-02, -7.4130e-01,\n",
      "                   -4.9262e-01,  5.0598e-01,  9.9487e-01,  8.4030e-01,\n",
      "                   -3.4268e-01,  1.3690e+00, -1.2085e+00, -1.9668e-01,\n",
      "                   -1.1587e+00, -2.8554e-02,  3.9756e-04,  4.5752e-02],\n",
      "                  requires_grad=True))\n",
      "Fatto queste volte:  22\n",
      "Epoch 1 Loss: 2.4333410263061523 x: -0.8503703474998474 f(x): 1e-05\n",
      "Fatto queste volte:  0\n",
      "Epoch 51 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 101 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 151 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 201 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 251 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 301 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 351 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 401 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 451 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 501 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Epoch 551 Loss: 0.0 x: -0.8126951456069946 f(x): 1e-05\n",
      "Punto finale: Parameter on Caste manifold containing:\n",
      "Parameter(ManifoldParameter([ 0.2025, -0.8127,  1.2178,  0.1256,  0.0895,  0.4443,\n",
      "                    0.5006, -0.1717, -1.4330, -0.1776, -0.6378,  0.8386,\n",
      "                   -0.1822,  0.2342,  0.5463,  1.0575,  0.5047,  0.0361,\n",
      "                   -0.7880, -0.0892,  0.6824,  0.2710, -0.1410,  0.1069,\n",
      "                   -0.8751, -0.3731, -1.0876, -1.1292,  0.1990,  0.8131,\n",
      "                    1.6065,  0.8780,  0.3522,  1.6575, -0.1884,  0.9434,\n",
      "                   -0.4573,  0.0436,  1.7430,  0.0206, -0.0080, -1.3300,\n",
      "                   -0.1348,  0.1881, -0.4957,  0.0287, -1.0841, -1.0713,\n",
      "                   -0.1530, -0.8838,  0.0628, -0.6762, -0.4878,  0.5443,\n",
      "                    0.9599,  0.8000, -0.3095,  1.2674, -1.1710, -0.2095,\n",
      "                   -1.2888,  0.0736,  0.1155, -0.0301], requires_grad=True))\n",
      "Appartiene alla varietà? tensor(True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creiamo la varietà\n",
    "manifold = LevelSetManifold(f, df, 1)\n",
    "\n",
    "# Test del parametro manifold\n",
    "x = geoopt.ManifoldParameter((torch.rand(64).float() - 0.5)*2 , manifold=manifold)\n",
    "\n",
    "# print(\"Param Manifold inizializzato correttamente:\", x)\n",
    "\n",
    "\n",
    "# Proiettiamo il parametro iniziale sulla varietà\n",
    "fs = []\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "# print(\"99\", \"Primo x: \",  x.data)\n",
    "\n",
    "for i in range(100):\n",
    "    x.data = manifold.projx(x.data)\n",
    "    if i%50 == 0:\n",
    "#         print(i, \"X: \",  x.data)\n",
    "        print(i, \"f: \", f(x).item())\n",
    "        print(i, \"grad: \", df(x).norm().item())\n",
    "\n",
    "        if f(x)< 1e-5:\n",
    "            break\n",
    "    fs.append(f(x).item())\n",
    "# plt.plot(fs)\n",
    "# Verifica che il parametro iniziale appartenga alla varietà\n",
    "assert manifold._check_point_on_manifold(x.data), \"Il punto iniziale non è sulla varietà\"\n",
    "\n",
    "# Definiamo una loss function: minimizziamo la norma quadrata\n",
    "def loss_fn(x):\n",
    "    return (-x[1] - torch.sin(x[2]) + x[3])**2  # Ad esempio, massimizzare la componente x[0]\n",
    "\n",
    "# Ottimizzatore Riemanniano\n",
    "optimizer = RiemannianSGD([x], lr=0.3)\n",
    "\n",
    "# Ciclo di ottimizzazione\n",
    "f_during_train = []\n",
    "loss_history = []\n",
    "print(\"Punto iniziale:\", x)\n",
    "for epoch in range(600):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%50 == 0:\n",
    "        print(\"Epoch\",  epoch + 1, \n",
    "              \"Loss:\", loss.item(),\n",
    "              \"x:\", x.data[1].item(),\n",
    "              \"f(x):\", round(float(f(x).data.numpy()),5))\n",
    "    loss_history.append(loss.item())\n",
    "    f_during_train.append(f(x).data.numpy())\n",
    "plt.plot(f_during_train)\n",
    "# plt.plot(loss_history)\n",
    "# Risultato finale\n",
    "print(\"Punto finale:\", x)\n",
    "print(\"Appartiene alla varietà?\", manifold._check_point_on_manifold(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7d772bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14e823d0ca0>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2de92",
   "metadata": {},
   "source": [
    "TODO list:\n",
    "* tolerance depends only on a treshold that we have to choose depending on the invariance\n",
    "* guarantee that reiterated retraction projects on the manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e27288",
   "metadata": {},
   "source": [
    "# Real NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55833b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.func import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4302dd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1929e-02,  2.4367e-02,  5.4129e-03,  2.3261e-03,  4.2511e-03,\n",
       "         1.1250e-02,  1.1864e-02,  5.4683e-03,  1.1347e-02,  9.2166e-03,\n",
       "         1.5401e-02, -4.4390e-03,  2.5588e-02, -5.7110e-03,  1.6551e-02,\n",
       "         5.6997e-03, -4.5344e-03, -5.0760e-03, -9.6055e-04,  2.0020e-02,\n",
       "         2.2096e-02,  2.1225e-02, -1.1016e-03,  2.0093e-04, -3.7076e-03,\n",
       "         1.5860e-02,  9.2735e-03,  1.0441e-02,  1.5576e-02, -5.4684e-03,\n",
       "        -5.0048e-03, -2.3941e-02,  1.4133e-02,  1.8573e-02,  6.0208e-05,\n",
       "         8.0895e-03,  2.3702e-02,  8.4699e-03,  5.3998e-03, -8.8548e-03,\n",
       "        -5.5279e-03,  3.0612e-03, -1.3604e-03, -7.4621e-05,  6.8390e-03,\n",
       "        -3.6220e-03,  7.1152e-03,  7.8312e-03, -4.5312e-03,  8.8637e-03,\n",
       "        -4.1083e-03,  1.0307e-02, -2.0713e-05,  1.9426e-02,  3.6954e-03,\n",
       "         2.5964e-02,  5.5866e-03,  8.4100e-03,  2.5605e-03, -7.6902e-03,\n",
       "        -1.9549e-02, -1.0406e-02, -3.1252e-02, -4.7869e-03],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand((10,64)).float()/10\n",
    "b = torch.rand((1,10)).float()/10\n",
    "\n",
    "W1 = torch.rand((10,10)).float()/10\n",
    "b1 = torch.rand((1,10)).float()/10\n",
    "\n",
    "W2 = torch.rand((10,10)).float()/10\n",
    "b2 = torch.rand((1,10)).float()/10\n",
    "\n",
    "W3 = torch.rand((10,64)).float()/10\n",
    "b3 = torch.rand((1,10)).float()/10\n",
    "\n",
    "dg(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "986be481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che definisce la varietà: sfera unitaria centrata in 0\n",
    "def g(theta):\n",
    "    return ((torch.tanh(W1 @ torch.tanh(W @ theta + b).T + b1.T)-torch.tanh(W2 @ torch.tanh(W3 @ theta + b3).T + b2.T)).norm())**2  # Sfera unitaria\n",
    "\n",
    "# Gradiente di f\n",
    "def dg(theta):\n",
    "    gradient = grad(g)(theta)\n",
    "    return gradient #+ 1e-5*(gradient.norm()==0)\n",
    "\n",
    "# Classe LevelSetManifold già implementata sopra\n",
    "class LevelSetManifold(geoopt.manifolds.Manifold):\n",
    "    \n",
    "    ndim = 1\n",
    "    name = \"Caste\"\n",
    "    \n",
    "    def __init__(self, f, df, lr_proj = 1):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "        self.lr_proj = lr_proj\n",
    "\n",
    "    def _check_point_on_manifold(self, x, atol=1e-7, rtol=1e-7):\n",
    "        return torch.abs(self.f(x)) < atol\n",
    "\n",
    "    def _check_vector_on_tangent(self, x, u, atol=1e-7, rtol=1e-7):\n",
    "        grad_f = self.df(x)\n",
    "        return torch.abs(u @ grad_f).sum() < atol\n",
    "    \n",
    "    def projx(self,x):\n",
    "        if self._check_point_on_manifold(x):\n",
    "            return x\n",
    "        for r in range(50):\n",
    "            x = self.single_projx(x)\n",
    "            if r == 49:\n",
    "                print(f\"Retraction applied {r + 1} times\")\n",
    "            if self._check_point_on_manifold(x):\n",
    "                print(f\"Retraction applied {r + 1} times\")\n",
    "                break\n",
    "        return x\n",
    "    \n",
    "    def single_projx(self, x):\n",
    "        grad_f = self.df(x)\n",
    "        f_val = self.f(x)\n",
    "        return x - self.lr_proj*(f_val / grad_f.norm()**2 * grad_f)\n",
    "\n",
    "    def proju(self, x, u):\n",
    "        grad_f = self.df(x)\n",
    "        return u - (u @ grad_f) / grad_f.norm()**2 * grad_f\n",
    "\n",
    "    def inner(self, x, u, v=None):\n",
    "        if v is None:\n",
    "            v = u\n",
    "        return (u * v).sum()\n",
    "\n",
    "    def expmap(self, x, u):\n",
    "        return self.retr(x, u)\n",
    "\n",
    "    def egrad2rgrad(self, x, u):\n",
    "        return self.proju(x, u)\n",
    "\n",
    "    def retr(self, x, u):\n",
    "        x_new = x + u\n",
    "        return self.projx(x_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa4bc424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############ Landing phase #############\n",
      "Retraction applied 50 times\n",
      "0 g:  0.0004681858990807086\n",
      "Retraction applied 50 times\n",
      "1 g:  1.4931812074792106e-05\n",
      "Retraction applied 50 times\n",
      "2 g:  3.374856987647945e-06\n",
      "Retraction applied 21 times\n",
      "3 g:  6.259582363554728e-08\n",
      "Landed :)\n",
      "\n",
      "\n",
      "\n",
      "############## Training phase ################\n",
      "Retraction applied 28 times\n",
      "Epoch 1 Loss: 3.6583056449890137 g(theta): 1e-07\n",
      "Retraction applied 35 times\n",
      "Epoch 2 Loss: 2.6198463439941406 g(theta): 1e-07\n",
      "Retraction applied 34 times\n",
      "Epoch 3 Loss: 1.8829315900802612 g(theta): 1e-07\n",
      "Retraction applied 30 times\n",
      "Epoch 4 Loss: 1.3552013635635376 g(theta): 7e-08\n",
      "Retraction applied 26 times\n",
      "Epoch 5 Loss: 0.9814071655273438 g(theta): 1e-07\n",
      "Retraction applied 19 times\n",
      "Epoch 6 Loss: 0.7072015404701233 g(theta): 1e-07\n",
      "Retraction applied 18 times\n",
      "Epoch 7 Loss: 0.5129783153533936 g(theta): 9e-08\n",
      "Retraction applied 21 times\n",
      "Epoch 8 Loss: 0.3738875091075897 g(theta): 9e-08\n",
      "Retraction applied 11 times\n",
      "Epoch 9 Loss: 0.27203696966171265 g(theta): 9e-08\n",
      "Retraction applied 15 times\n",
      "Epoch 10 Loss: 0.19843733310699463 g(theta): 1e-07\n",
      "Retraction applied 13 times\n",
      "Epoch 11 Loss: 0.14544858038425446 g(theta): 1e-07\n",
      "Retraction applied 13 times\n",
      "Epoch 12 Loss: 0.10632596909999847 g(theta): 1e-07\n",
      "Retraction applied 10 times\n",
      "Epoch 13 Loss: 0.0786532610654831 g(theta): 9e-08\n",
      "Retraction applied 13 times\n",
      "Epoch 14 Loss: 0.058099932968616486 g(theta): 9e-08\n",
      "Retraction applied 9 times\n",
      "Epoch 15 Loss: 0.0425574854016304 g(theta): 7e-08\n",
      "Retraction applied 2 times\n",
      "Epoch 16 Loss: 0.031741902232170105 g(theta): 1e-07\n",
      "Retraction applied 3 times\n",
      "Epoch 17 Loss: 0.023288121446967125 g(theta): 8e-08\n",
      "Retraction applied 2 times\n",
      "Epoch 18 Loss: 0.017721712589263916 g(theta): 9e-08\n",
      "Retraction applied 5 times\n",
      "Epoch 19 Loss: 0.0124735739082098 g(theta): 9e-08\n",
      "Retraction applied 5 times\n",
      "Epoch 20 Loss: 0.00931389071047306 g(theta): 9e-08\n",
      "Retraction applied 9 times\n",
      "Epoch 21 Loss: 0.006818034220486879 g(theta): 8e-08\n",
      "Retraction applied 3 times\n",
      "Epoch 22 Loss: 0.00512844929471612 g(theta): 9e-08\n",
      "Retraction applied 3 times\n",
      "Epoch 23 Loss: 0.00371544505469501 g(theta): 9e-08\n",
      "Retraction applied 4 times\n",
      "Epoch 24 Loss: 0.0027693090960383415 g(theta): 9e-08\n",
      "Retraction applied 2 times\n",
      "Epoch 25 Loss: 0.002059690188616514 g(theta): 1e-07\n",
      "Retraction applied 3 times\n",
      "Epoch 26 Loss: 0.0015404982259497046 g(theta): 8e-08\n",
      "Epoch 27 Loss: 0.0011826619738712907 g(theta): 9e-08\n",
      "Retraction applied 2 times\n",
      "Epoch 28 Loss: 0.0008453502086922526 g(theta): 9e-08\n",
      "Epoch 29 Loss: 0.0006527724326588213 g(theta): 9e-08\n",
      "Epoch 30 Loss: 0.0004562309186439961 g(theta): 1e-07\n",
      "Retraction applied 3 times\n",
      "Epoch 31 Loss: 0.00031964192749001086 g(theta): 7e-08\n",
      "Epoch 32 Loss: 0.0003004215541295707 g(theta): 7e-08\n",
      "Epoch 33 Loss: 0.00021980142628308386 g(theta): 7e-08\n",
      "Epoch 34 Loss: 0.00016038194007705897 g(theta): 7e-08\n",
      "Epoch 35 Loss: 0.00011459945380920544 g(theta): 7e-08\n",
      "Epoch 36 Loss: 8.38321284390986e-05 g(theta): 7e-08\n",
      "Epoch 37 Loss: 6.185559323057532e-05 g(theta): 7e-08\n",
      "Epoch 38 Loss: 4.567344512906857e-05 g(theta): 7e-08\n",
      "Epoch 39 Loss: 3.3667765819700435e-05 g(theta): 7e-08\n",
      "Epoch 40 Loss: 2.4878507247194648e-05 g(theta): 7e-08\n",
      "Epoch 41 Loss: 1.842443089117296e-05 g(theta): 7e-08\n",
      "Epoch 42 Loss: 1.3664562175108586e-05 g(theta): 7e-08\n",
      "Epoch 43 Loss: 1.0137607205251697e-05 g(theta): 7e-08\n",
      "Epoch 44 Loss: 7.522118721681181e-06 g(theta): 7e-08\n",
      "Epoch 45 Loss: 5.580794550041901e-06 g(theta): 7e-08\n",
      "Epoch 46 Loss: 4.141798854107037e-06 g(theta): 7e-08\n",
      "Epoch 47 Loss: 3.0733308449271135e-06 g(theta): 7e-08\n",
      "Epoch 48 Loss: 2.2805327262176434e-06 g(theta): 7e-08\n",
      "Epoch 49 Loss: 1.692111254669726e-06 g(theta): 7e-08\n",
      "Epoch 50 Loss: 1.2559382867038948e-06 g(theta): 7e-08\n",
      "Epoch 51 Loss: 9.319138030150498e-07 g(theta): 7e-08\n",
      "Epoch 52 Loss: 6.913669494679198e-07 g(theta): 7e-08\n",
      "Epoch 53 Loss: 5.132974933985679e-07 g(theta): 7e-08\n",
      "Epoch 54 Loss: 3.807226676144637e-07 g(theta): 7e-08\n",
      "Epoch 55 Loss: 2.826766376529122e-07 g(theta): 7e-08\n",
      "Epoch 56 Loss: 2.0976591486032703e-07 g(theta): 7e-08\n",
      "Epoch 57 Loss: 1.5560148369786475e-07 g(theta): 7e-08\n",
      "Epoch 58 Loss: 1.1558972801140044e-07 g(theta): 7e-08\n",
      "Epoch 59 Loss: 8.571896614739671e-08 g(theta): 7e-08\n",
      "Epoch 60 Loss: 6.36284767097095e-08 g(theta): 7e-08\n",
      "Epoch 61 Loss: 4.717554702438065e-08 g(theta): 7e-08\n",
      "Epoch 62 Loss: 3.507297208216187e-08 g(theta): 7e-08\n",
      "Epoch 63 Loss: 2.6052987323055277e-08 g(theta): 7e-08\n",
      "Epoch 64 Loss: 1.9320452793181175e-08 g(theta): 7e-08\n",
      "Epoch 65 Loss: 1.4324768926599063e-08 g(theta): 7e-08\n",
      "Epoch 66 Loss: 1.0657515758794034e-08 g(theta): 7e-08\n",
      "Epoch 67 Loss: 7.908568022685358e-09 g(theta): 7e-08\n",
      "Epoch 68 Loss: 5.8389701962369145e-09 g(theta): 7e-08\n",
      "Epoch 69 Loss: 4.330104275140911e-09 g(theta): 7e-08\n",
      "Epoch 70 Loss: 3.2333815624951967e-09 g(theta): 7e-08\n",
      "Epoch 71 Loss: 2.3772059876137064e-09 g(theta): 7e-08\n",
      "Epoch 72 Loss: 1.760781742632389e-09 g(theta): 7e-08\n",
      "Epoch 73 Loss: 1.3046843605479808e-09 g(theta): 7e-08\n",
      "Epoch 74 Loss: 9.606537787476555e-10 g(theta): 7e-08\n",
      "Epoch 75 Loss: 7.130438461899757e-10 g(theta): 7e-08\n",
      "Epoch 76 Loss: 5.348397280613426e-10 g(theta): 7e-08\n",
      "Epoch 77 Loss: 3.915943125321064e-10 g(theta): 7e-08\n",
      "Epoch 78 Loss: 2.9059776807116577e-10 g(theta): 7e-08\n",
      "Epoch 79 Loss: 2.1499602098629111e-10 g(theta): 7e-08\n",
      "Epoch 80 Loss: 1.5967316358000971e-10 g(theta): 7e-08\n",
      "Epoch 81 Loss: 1.2028067430946976e-10 g(theta): 7e-08\n",
      "Epoch 82 Loss: 8.86899442775757e-11 g(theta): 7e-08\n",
      "Epoch 83 Loss: 6.571099220309407e-11 g(theta): 7e-08\n",
      "Epoch 84 Loss: 4.780531526193954e-11 g(theta): 7e-08\n",
      "Epoch 85 Loss: 3.842615114990622e-11 g(theta): 7e-08\n",
      "Epoch 86 Loss: 2.751221472863108e-11 g(theta): 7e-08\n",
      "Epoch 87 Loss: 2.0520474208751693e-11 g(theta): 7e-08\n",
      "Epoch 88 Loss: 1.4551915228366852e-11 g(theta): 7e-08\n",
      "Epoch 89 Loss: 1.1141310096718371e-11 g(theta): 7e-08\n",
      "Epoch 90 Loss: 8.185452315956354e-12 g(theta): 7e-08\n",
      "Epoch 91 Loss: 6.266986929404084e-12 g(theta): 7e-08\n",
      "Epoch 92 Loss: 4.604316927725449e-12 g(theta): 7e-08\n",
      "Epoch 93 Loss: 3.197442310920451e-12 g(theta): 7e-08\n",
      "Epoch 94 Loss: 2.4016344468691386e-12 g(theta): 7e-08\n",
      "Epoch 95 Loss: 1.7195134205394424e-12 g(theta): 7e-08\n",
      "Epoch 96 Loss: 1.1510792319313623e-12 g(theta): 7e-08\n",
      "Epoch 97 Loss: 9.094947017729282e-13 g(theta): 7e-08\n",
      "Epoch 98 Loss: 6.963318810448982e-13 g(theta): 7e-08\n",
      "Epoch 99 Loss: 6.963318810448982e-13 g(theta): 7e-08\n",
      "Epoch 100 Loss: 6.963318810448982e-13 g(theta): 7e-08\n",
      "Epoch 101 Loss: 5.115907697472721e-13 g(theta): 7e-08\n",
      "Epoch 102 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 103 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 104 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 105 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 106 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 107 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 108 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 109 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 110 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 111 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 112 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 113 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 114 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 115 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 116 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 117 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 118 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 119 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 120 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 121 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 122 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 123 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 124 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 125 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 126 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 127 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 128 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 129 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 130 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 131 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 132 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 133 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 134 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 135 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 136 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 137 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 138 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 139 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 140 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 141 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 142 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 143 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 144 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 145 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 146 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 147 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 148 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 149 Loss: 3.552713678800501e-13 g(theta): 7e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 151 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 152 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 153 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 154 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 155 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 156 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 157 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 158 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 159 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 160 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 161 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 162 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 163 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 164 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 165 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 166 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 167 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 168 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 169 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 170 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 171 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 172 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 173 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 174 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 175 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 176 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 177 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 178 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 179 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 180 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 181 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 182 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 183 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 184 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 185 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 186 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 187 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 188 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 189 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 190 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 191 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 192 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 193 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 194 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 195 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 196 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 197 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 198 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 199 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 200 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 201 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 202 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 203 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 204 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 205 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 206 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 207 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 208 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 209 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 210 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 211 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 212 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 213 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 214 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 215 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 216 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 217 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 218 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 219 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 220 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 221 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 222 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 223 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 224 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 225 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 226 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 227 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 228 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 229 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 230 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 231 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 232 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 233 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 234 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 235 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 236 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 237 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 238 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 239 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 240 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 241 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 242 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 243 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 244 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 245 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 246 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 247 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 248 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 249 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 250 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 251 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 252 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 253 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 254 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 255 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 256 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 257 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 258 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 259 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 260 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 261 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 262 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 263 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 264 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 265 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 266 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 267 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 268 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 269 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 270 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 271 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 272 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 273 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 274 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 275 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 276 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 277 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 278 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 279 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 280 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 281 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 282 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 283 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 284 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 285 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 286 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 287 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 288 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 289 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 290 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 291 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 292 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 293 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 294 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 295 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 296 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 297 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 298 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 299 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 300 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 301 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 302 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 303 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 304 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 305 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 306 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 307 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 308 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 309 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 310 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 311 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 312 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 313 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 314 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 315 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 316 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 317 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 318 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 319 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 320 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 321 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 322 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 323 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 324 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 325 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 326 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 327 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 328 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 329 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 330 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 331 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 332 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 333 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 334 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 335 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 336 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 337 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 338 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 339 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 340 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 341 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 342 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 343 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 344 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 345 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 346 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 347 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 348 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 349 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 350 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 351 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 352 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 353 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 354 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 355 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 356 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 357 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 358 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 359 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 360 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 361 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 362 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 363 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 364 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 365 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 366 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 367 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 368 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 369 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 370 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 371 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 372 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 373 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 374 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 375 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 376 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 377 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 378 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 379 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 380 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 381 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 382 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 383 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 384 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 385 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 386 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 387 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 388 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 389 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 390 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 391 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 392 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 393 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 394 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 395 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 396 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 397 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 398 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 399 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 400 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 401 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 402 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 403 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 404 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 405 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 406 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 407 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 408 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 409 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 410 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 411 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 412 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 413 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 414 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 415 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 416 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 417 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 418 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 419 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 420 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 421 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 422 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 423 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 424 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 425 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 426 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 427 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 428 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 429 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 430 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 431 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 432 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 433 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 434 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 435 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 436 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 437 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 438 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 439 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 440 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 441 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 442 Loss: 3.552713678800501e-13 g(theta): 7e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 444 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 445 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 446 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 447 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 448 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 449 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 450 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 451 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 452 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 453 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 454 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 455 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 456 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 457 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 458 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 459 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 460 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 461 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 462 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 463 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 464 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 465 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 466 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 467 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 468 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 469 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 470 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 471 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 472 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 473 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 474 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 475 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 476 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 477 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 478 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 479 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 480 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 481 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 482 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 483 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 484 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 485 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 486 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 487 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 488 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 489 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 490 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 491 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 492 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 493 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 494 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 495 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 496 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 497 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 498 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 499 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 500 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 501 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 502 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 503 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 504 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 505 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 506 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 507 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 508 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 509 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 510 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 511 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 512 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 513 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 514 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 515 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 516 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 517 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 518 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 519 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 520 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 521 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 522 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 523 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 524 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 525 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 526 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 527 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 528 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 529 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 530 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 531 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 532 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 533 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 534 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 535 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 536 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 537 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 538 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 539 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 540 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 541 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 542 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 543 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 544 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 545 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 546 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 547 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 548 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 549 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 550 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 551 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 552 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 553 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 554 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 555 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 556 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 557 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 558 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 559 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 560 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 561 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 562 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 563 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 564 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 565 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 566 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 567 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 568 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 569 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 570 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 571 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 572 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 573 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 574 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 575 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 576 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 577 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 578 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 579 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 580 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 581 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 582 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 583 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 584 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 585 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 586 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 587 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 588 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 589 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 590 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 591 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 592 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 593 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 594 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 595 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 596 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 597 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 598 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 599 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "Epoch 600 Loss: 3.552713678800501e-13 g(theta): 7e-08\n",
      "\n",
      "\n",
      "\n",
      "Punto finale: Parameter on Caste manifold containing:\n",
      "Parameter(ManifoldParameter([-0.9400,  1.2088,  0.5482,  1.7300, -0.2593, -0.8128,\n",
      "                    1.1156,  0.8971, -0.6417,  0.3475,  1.3352, -2.4665,\n",
      "                    0.7988, -0.2901, -0.8107,  1.0150, -0.3411,  0.0892,\n",
      "                   -1.2018, -0.1667,  1.1392, -0.6198, -0.1453, -1.2717,\n",
      "                    0.9008,  1.4423, -0.0181,  1.5831, -0.3267, -1.3905,\n",
      "                    0.4900, -0.3200,  0.0314, -0.0371,  0.9910,  0.1768,\n",
      "                    1.5591, -2.1320, -1.0941,  0.2339, -0.2198, -0.5483,\n",
      "                    0.5180,  0.3120, -1.5255,  2.5908, -1.0026,  0.8592,\n",
      "                   -1.7441, -0.8388, -0.8100, -1.3547, -0.2720,  0.4012,\n",
      "                    0.3856,  0.2936,  1.0851, -0.0065,  0.7723, -0.4165,\n",
      "                    1.0140,  1.0678, -2.8826,  0.8316], requires_grad=True))\n",
      "Appartiene alla varietà? tensor(True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creiamo la varietà\n",
    "manifold = LevelSetManifold(g, dg, 1)\n",
    "# Test del parametro manifold\n",
    "theta = geoopt.ManifoldParameter((torch.rand(64).float() - 0.5)*2 , manifold=manifold)\n",
    "\n",
    "\n",
    "# Proiettiamo il parametro iniziale sulla varietà\n",
    "gs = []\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "# print(\"99\", \"Primo theta: \",  theta.data)\n",
    "\n",
    "print(\"############ Landing phase #############\")\n",
    "for i in range(100):\n",
    "    theta.data = manifold.projx(theta.data)\n",
    "    if i%1 == 0:\n",
    "        print(i, \"g: \", g(theta).item())\n",
    "\n",
    "    if g(theta)< 1e-7:\n",
    "        print(\"Landed :)\")\n",
    "        break\n",
    "            \n",
    "#     gs.append(g(theta).item())\n",
    "    \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Verifica che il parametro iniziale appartenga alla varietà\n",
    "assert manifold._check_point_on_manifold(theta.data), \"Il punto iniziale non è sulla varietà\"\n",
    "\n",
    "# Definiamo una loss function: minimizziamo la norma quadrata\n",
    "def loss_fn(theta):\n",
    "    return (-theta[1] - torch.sin(theta[2]) + theta[3])**2  # Ad esempio, massimizzare la componente x[0]\n",
    "\n",
    "# Ottimizzatore Riemanniano\n",
    "optimizer = RiemannianSGD([theta], lr=0.03)\n",
    "\n",
    "# Ciclo di ottimizzazione\n",
    "g_during_train = []\n",
    "loss_history = []\n",
    "print(\"############## Training phase ################\")\n",
    "for epoch in range(600):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(theta)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%1 == 0:\n",
    "        print(\"Epoch\",  epoch + 1, \n",
    "              \"Loss:\", loss.item(),\n",
    "#               \"theta:\", theta.data[1].item(),\n",
    "              \"g(theta):\", round(float(g(theta).data.numpy()),8))\n",
    "    loss_history.append(loss.item())\n",
    "    g_during_train.append(g(theta).data.numpy())\n",
    "plt.plot(g_during_train)\n",
    "# plt.plot(loss_history)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Risultato finale\n",
    "print(\"Punto finale:\", theta)\n",
    "print(\"Appartiene alla varietà?\", manifold._check_point_on_manifold(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f991f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
